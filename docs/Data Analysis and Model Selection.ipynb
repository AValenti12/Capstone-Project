{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add1726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# null hypothesis: all coefficients in the model are equal to zero \n",
    "# (predictor variables have a statistically significant relationship with y)\n",
    "# alternative hypothesis: not every coefficient is equal to zero\n",
    "\n",
    "# we are using 21 predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2abf107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder,RobustScaler,PowerTransformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer,Binarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,SGDRegressor\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from config import db_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "036d50e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annasand/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "/Users/annasand/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/ipykernel_launcher.py:29: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    }
   ],
   "source": [
    "client = pymongo.MongoClient(f\"mongodb+srv://nickj:{db_password}@capstonecluster.7h9ij.mongodb.net/CapstoneCluster?retryWrites=true&w=majority\")\n",
    "\n",
    "db = client.listings_db\n",
    "collection1 = db.X_set.find()\n",
    "\n",
    "collection2 = db.y_set.find()\n",
    "\n",
    "X = pd.DataFrame(collection1)\n",
    "y = pd.DataFrame(collection2)\n",
    "\n",
    "X = X.drop('_id',1)\n",
    "X['price'] = X['price'].div(100)\n",
    "X['amenities'] = X['amenities'].div(10)\n",
    "X['maximum_nights'] = X['maximum_nights'].div(100)\n",
    "y_new = []\n",
    "for i in y['is_success'][0]:\n",
    "    if i == 'yes':\n",
    "        y_new.append(1)\n",
    "    else:\n",
    "        y_new.append(0)\n",
    "        \n",
    "y = y_new\n",
    "\n",
    "X['is_success']=y\n",
    "\n",
    "X = X.sort_values(by=['maximum_nights'],ascending=False)[3:]\n",
    "\n",
    "y = X['is_success']\n",
    "X = X.drop('is_success',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "64e07bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>amenities</th>\n",
       "      <th>price</th>\n",
       "      <th>security_deposit</th>\n",
       "      <th>cleaning_fee</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>extra_people</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>maximum_nights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "      <td>15960.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.305388</td>\n",
       "      <td>1.170990</td>\n",
       "      <td>1.273810</td>\n",
       "      <td>1.796554</td>\n",
       "      <td>0.240799</td>\n",
       "      <td>1.546612</td>\n",
       "      <td>2.548427</td>\n",
       "      <td>0.694052</td>\n",
       "      <td>1.803133</td>\n",
       "      <td>0.201096</td>\n",
       "      <td>5.909586</td>\n",
       "      <td>5.735107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.172312</td>\n",
       "      <td>0.505956</td>\n",
       "      <td>0.840775</td>\n",
       "      <td>1.294196</td>\n",
       "      <td>0.098599</td>\n",
       "      <td>1.781979</td>\n",
       "      <td>4.475486</td>\n",
       "      <td>0.562435</td>\n",
       "      <td>1.390159</td>\n",
       "      <td>0.254423</td>\n",
       "      <td>15.275039</td>\n",
       "      <td>10.620205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.230000</td>\n",
       "      <td>1.160000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>11.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.000000</td>\n",
       "      <td>16.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1100.000000</td>\n",
       "      <td>999.990000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       accommodates     bathrooms      bedrooms          beds     amenities  \\\n",
       "count  15960.000000  15960.000000  15960.000000  15960.000000  15960.000000   \n",
       "mean       3.305388      1.170990      1.273810      1.796554      0.240799   \n",
       "std        2.172312      0.505956      0.840775      1.294196      0.098599   \n",
       "min        1.000000      0.000000      0.000000      0.000000      0.010000   \n",
       "25%        2.000000      1.000000      1.000000      1.000000      0.170000   \n",
       "50%        2.000000      1.000000      1.000000      1.000000      0.230000   \n",
       "75%        4.000000      1.000000      1.000000      2.000000      0.300000   \n",
       "max       16.000000     16.500000     14.000000     40.000000      0.820000   \n",
       "\n",
       "              price  security_deposit  cleaning_fee  guests_included  \\\n",
       "count  15960.000000      15960.000000  15960.000000     15960.000000   \n",
       "mean       1.546612          2.548427      0.694052         1.803133   \n",
       "std        1.781979          4.475486      0.562435         1.390159   \n",
       "min        0.000000          0.000000      0.000000         1.000000   \n",
       "25%        0.720000          0.000000      0.300000         1.000000   \n",
       "50%        1.160000          1.250000      0.600000         1.000000   \n",
       "75%        1.850000          3.000000      1.000000         2.000000   \n",
       "max       85.000000         51.000000      6.000000        16.000000   \n",
       "\n",
       "       extra_people  minimum_nights  maximum_nights  \n",
       "count  15960.000000    15960.000000    15960.000000  \n",
       "mean       0.201096        5.909586        5.735107  \n",
       "std        0.254423       15.275039       10.620205  \n",
       "min        0.000000        1.000000        0.010000  \n",
       "25%        0.000000        1.000000        0.300000  \n",
       "50%        0.150000        2.000000        3.650000  \n",
       "75%        0.250000        4.000000       11.250000  \n",
       "max        3.000000     1100.000000      999.990000  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there seem to be some outliers in some columns in continuous variables\n",
    "cont = X[['accommodates', 'bathrooms', 'bedrooms',\n",
    "       'beds', 'amenities', 'price', 'security_deposit',\n",
    "       'cleaning_fee', 'guests_included', 'extra_people', 'minimum_nights',\n",
    "       'maximum_nights']]\n",
    "cont.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4283d49e",
   "metadata": {},
   "source": [
    "## Splitting Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "456ee427",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14364, 21)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  test_size=0.1, random_state=0)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9741446",
   "metadata": {},
   "source": [
    "## Testing Different Scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c489ccf",
   "metadata": {},
   "source": [
    "### Scaled data has zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f93746d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a StandardScaler instance.\n",
    "scaler = StandardScaler()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_ss = X_scaler.transform(X_train)\n",
    "test_ss = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18e30a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.90200280e-16, -4.67462326e-17,  2.10234379e-17,  2.67121329e-17,\n",
       "         2.37441182e-17,  2.87897433e-16, -4.45202215e-16,  4.25415450e-17,\n",
       "        -4.40255524e-17,  1.85006254e-16,  1.87974269e-17, -1.03633182e-16,\n",
       "        -9.89338256e-18, -8.75564357e-17, -3.19061588e-17,  0.00000000e+00,\n",
       "        -3.46268390e-17,  8.60724283e-17,  6.72750014e-17,  2.63658645e-16,\n",
       "        -1.73628864e-16]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
       "        1., 1., 1., 1.]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled data mean and variance\n",
    "X_ss.mean(axis=0),X_ss.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d2f4ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScaler\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "transformer\n",
    "\n",
    "train_Ro = transformer.transform(X_train)\n",
    "test_Ro = transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "31036d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.4796714 ,  0.65159426,  0.17091339,  0.27464495,  0.79727096,\n",
       "        -0.03411306,  0.08689459,  0.3485804 ,  0.4342465 ,  0.13395393,\n",
       "         0.79741019,  0.20451685,  1.32075559,  0.19180015,  0.39452799,\n",
       "         0.        ,  0.53933445,  0.03808131,  0.03968254,  0.25160123,\n",
       "        -0.44075466]),\n",
       " array([0.49987279, 1.09611894, 0.51722213, 0.79465099, 1.20428526,\n",
       "        0.26666935, 0.75514161, 1.32718865, 1.5124836 , 0.83940956,\n",
       "        1.54227716, 0.95242779, 3.09838615, 0.48238698, 0.49047746,\n",
       "        0.        , 0.73230793, 0.18868182, 0.18868182, 0.8565619 ,\n",
       "        0.53441194]))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled data mean and variance\n",
    "train_Ro.mean(axis=0),test_Ro.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7ba6f236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 1., 1.],\n",
       "       [0., 1., 1., ..., 0., 1., 0.],\n",
       "       [0., 1., 1., ..., 0., 1., 1.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 1., 1.],\n",
       "       [1., 1., 1., ..., 0., 1., 1.],\n",
       "       [0., 1., 1., ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bins into 1s and 0s default threshold=0.0\n",
    "bin = Binarizer()\n",
    "\n",
    "train_bin = bin.fit_transform(X_train)\n",
    "test_bin = bin.fit_transform(X_test)\n",
    "\n",
    "train_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "319c451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.5203286 , 1.        , 0.99839877, 0.91033138, 0.99227235,\n",
       "        0.99895572, 1.        , 0.9992342 , 0.64174325, 0.93964077,\n",
       "        1.        , 0.65559733, 1.        , 1.        , 0.39452799,\n",
       "        0.        , 0.99993038, 0.03808131, 0.03968254, 0.98948761,\n",
       "        0.98085492]),\n",
       " array([0.49958658, 0.        , 0.03998326, 0.28570641, 0.08756675,\n",
       "        0.0322984 , 0.        , 0.02766255, 0.47948811, 0.2381512 ,\n",
       "        0.        , 0.4751731 , 0.        , 0.        , 0.48874907,\n",
       "        0.        , 0.00834348, 0.1913926 , 0.19521228, 0.10198962,\n",
       "        0.13703485]))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled data mean and variance:\n",
    "train_bin.mean(axis=0),train_bin.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "cfde69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "from path import Path\n",
    "from sklearn.preprocessing import RobustScaler,Binarizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f3023",
   "metadata": {},
   "source": [
    "## Logistic Regression with RobustScalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "639b10b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>9</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>6</td>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0            9          293\n",
       "Actual 1            6         1288"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.8126566416040101\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.03      0.06       302\n",
      "           1       0.81      1.00      0.90      1294\n",
      "\n",
      "    accuracy                           0.81      1596\n",
      "   macro avg       0.71      0.51      0.48      1596\n",
      "weighted avg       0.77      0.81      0.74      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  test_size=0.1, random_state=0)\n",
    "\n",
    "\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = transformer.transform(X_train)\n",
    "X_test_scaled = transformer.transform(X_test)\n",
    "\n",
    "# Create a random forest classifier.\n",
    "lgRo = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "\n",
    "# Fitting the model\n",
    "lgRo = lgRo.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = lgRo.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda3602b",
   "metadata": {},
   "source": [
    "## Random Forest with Binarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16babc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the results for this model were not impressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "4541db62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>9</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>3</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0            9          293\n",
       "Actual 1            3         1291"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.8145363408521303\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.03      0.06       302\n",
      "           1       0.82      1.00      0.90      1294\n",
      "\n",
      "    accuracy                           0.81      1596\n",
      "   macro avg       0.78      0.51      0.48      1596\n",
      "weighted avg       0.80      0.81      0.74      1596\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.2082498415115371, 'instant_bookable'),\n",
       " (0.14563061825558948, 'host_identity_verified'),\n",
       " (0.11357445362441397, 'extra_people'),\n",
       " (0.09630540600636277, 'security_deposit'),\n",
       " (0.0712107278441215, 'cleaning_fee'),\n",
       " (0.06657460338065607, 'require_guest_phone_verification'),\n",
       " (0.06425616793236923, 'bedrooms'),\n",
       " (0.06147492329862924, 'room'),\n",
       " (0.05715024800077582, 'require_guest_profile_picture'),\n",
       " (0.04333130852913801, 'beds'),\n",
       " (0.030283471079919244, 'neighbourhood'),\n",
       " (0.016277831848289907, 'bathrooms'),\n",
       " (0.013364477291118074, 'bed_type'),\n",
       " (0.01192000674963221, 'price'),\n",
       " (0.0003959146474474704, 'cancellation_policy'),\n",
       " (0.0, 'minimum_nights'),\n",
       " (0.0, 'maximum_nights'),\n",
       " (0.0, 'is_business_travel_ready'),\n",
       " (0.0, 'guests_included'),\n",
       " (0.0, 'amenities'),\n",
       " (0.0, 'accommodates')]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  test_size=0.1, random_state=0)\n",
    "\n",
    "# Creating a StandardScaler instance.\n",
    "scaler = Binarizer()\n",
    "# Fitting the Standard Scaler with the training data.\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=0) \n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# We can sort the features by their importance.\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b41b9d4",
   "metadata": {},
   "source": [
    "## RandomForestClassifier with RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model has given the best results so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "690f32a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>50</td>\n",
       "      <td>252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>32</td>\n",
       "      <td>1262</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0           50          252\n",
       "Actual 1           32         1262"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score : 0.8220551378446115\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.17      0.26       302\n",
      "           1       0.83      0.98      0.90      1294\n",
      "\n",
      "    accuracy                           0.82      1596\n",
      "   macro avg       0.72      0.57      0.58      1596\n",
      "weighted avg       0.79      0.82      0.78      1596\n",
      "\n",
      "Feature Importance : \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.14166282747357758, 'price'),\n",
       " (0.1310220998036831, 'amenities'),\n",
       " (0.10560037283436863, 'cleaning_fee'),\n",
       " (0.07788882462537369, 'maximum_nights'),\n",
       " (0.07399494176421915, 'extra_people'),\n",
       " (0.07313297725909008, 'minimum_nights'),\n",
       " (0.07141977963949586, 'security_deposit'),\n",
       " (0.0564625368165704, 'accommodates'),\n",
       " (0.038320898567455915, 'beds'),\n",
       " (0.03684102672923589, 'neighbourhood'),\n",
       " (0.035931274295566876, 'guests_included'),\n",
       " (0.02925441692359164, 'bedrooms'),\n",
       " (0.028693131728479225, 'cancellation_policy'),\n",
       " (0.025108370596539736, 'bathrooms'),\n",
       " (0.021786412976933536, 'host_identity_verified'),\n",
       " (0.01909101294544024, 'instant_bookable'),\n",
       " (0.015267973140351372, 'room'),\n",
       " (0.006506151742154061, 'require_guest_phone_verification'),\n",
       " (0.006085536424883748, 'bed_type'),\n",
       " (0.005929433712989231, 'require_guest_profile_picture'),\n",
       " (0.0, 'is_business_travel_ready')]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting into Train and Test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  test_size=0.1, random_state=0)\n",
    "\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "\n",
    "\n",
    "# Scaling the data.\n",
    "X_train_scaled = transformer.transform(X_train)\n",
    "X_test_scaled = transformer.transform(X_test)\n",
    "\n",
    "# Create a random forest classifier.\n",
    "rf_model = RandomForestClassifier(n_estimators=128, random_state=0) \n",
    "\n",
    "# Fitting the model\n",
    "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Making predictions using the testing data.\n",
    "predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculating the confusion matrix.\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "# Calculating the accuracy score.\n",
    "acc_score = accuracy_score(y_test, predictions)\n",
    "\n",
    "# Displaying results\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "print(f\"Accuracy Score : {acc_score}\")\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "# Calculate feature importance in the Random Forest model.\n",
    "importances = rf_model.feature_importances_\n",
    "\n",
    "# We can sort the features by their importance.\n",
    "print(f\"Feature Importance : \")\n",
    "\n",
    "sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b86229",
   "metadata": {},
   "source": [
    "## Setting training and test sets for more trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ac962a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({1: 11633, 0: 2731})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "   y,  test_size=0.1, random_state=1)\n",
    "\n",
    "transformer = RobustScaler().fit(X_train)\n",
    "#transformer = StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_scaled = transformer.transform(X_train)\n",
    "X_test_scaled = transformer.transform(X_test)\n",
    "\n",
    "Counter(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9dbd4b",
   "metadata": {},
   "source": [
    "## Oversampling (LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ba7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This model increased the precision for 1 but 0 is still not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "6973e9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>196</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>517</td>\n",
       "      <td>777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0          196          106\n",
       "Actual 1          517          777"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6247351505163925\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.27      0.65      0.60      0.39      0.62      0.39       302\n",
      "          1       0.88      0.60      0.65      0.71      0.62      0.39      1294\n",
      "\n",
      "avg / total       0.77      0.61      0.64      0.65      0.62      0.39      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1).fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d8c4d0",
   "metadata": {},
   "source": [
    "## SMOTE (LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "4d0b6d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6182226680450782\n",
      "[[170 132]\n",
      " [575 719]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.23      0.56      0.56      0.32      0.56      0.31       302\n",
      "          1       0.84      0.56      0.56      0.67      0.56      0.31      1294\n",
      "\n",
      "avg / total       0.73      0.56      0.56      0.60      0.56      0.31      1596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/annasand/opt/anaconda3/envs/mlenv/lib/python3.7/site-packages/sklearn/base.py:439: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  f\"X has feature names, but {self.__class__.__name__} was fitted without\"\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE(random_state=1,\n",
    "sampling_strategy='auto').fit_resample(\n",
    "   X_train_scaled, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782e3a2e",
   "metadata": {},
   "source": [
    "## UnderSampling (LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "21666047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194 108]\n",
      " [504 790]]\n",
      "0.6264470761640583\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.28      0.64      0.61      0.39      0.63      0.39       302\n",
      "          1       0.88      0.61      0.64      0.72      0.63      0.39      1294\n",
      "\n",
      "avg / total       0.77      0.62      0.64      0.66      0.63      0.39      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros = RandomUnderSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1).fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2db649e",
   "metadata": {},
   "source": [
    "## Cluster Centroid Undersampling (LogisticRegression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "0b377254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[229  73]\n",
      " [766 528]]\n",
      "0.5831576199883313\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.23      0.76      0.41      0.35      0.56      0.32       302\n",
      "          1       0.88      0.41      0.76      0.56      0.56      0.30      1294\n",
      "\n",
      "avg / total       0.76      0.47      0.69      0.52      0.56      0.30      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=1)\n",
    "X_resampled, y_resampled = cc.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc62f7cb",
   "metadata": {},
   "source": [
    "## SMOTEEN (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0d19fffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[257  45]\n",
      " [882 412]]\n",
      "0.584692979313592\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.23      0.85      0.32      0.36      0.52      0.29       302\n",
      "          1       0.90      0.32      0.85      0.47      0.52      0.26      1294\n",
      "\n",
      "avg / total       0.77      0.42      0.75      0.45      0.52      0.26      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = LogisticRegression(solver='lbfgs', random_state=1)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92900629",
   "metadata": {},
   "source": [
    "## Oversampling (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b4c5c335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>73</td>\n",
       "      <td>229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>66</td>\n",
       "      <td>1228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0           73          229\n",
       "Actual 1           66         1228"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5953586087597367\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.53      0.24      0.95      0.33      0.48      0.21       302\n",
      "          1       0.84      0.95      0.24      0.89      0.48      0.25      1294\n",
      "\n",
      "avg / total       0.78      0.82      0.38      0.79      0.48      0.24      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=128, random_state=0).fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Create a DataFrame from the confusion matrix.\n",
    "cm_df = pd.DataFrame(\n",
    "    cm, index=[\"Actual 0\", \"Actual 1\"], columns=[\"Predicted 0\", \"Predicted 1\"])\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "display(cm_df)\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee7bfb",
   "metadata": {},
   "source": [
    "## SMOTE (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3f496c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5976207048322876\n",
      "[[  76  226]\n",
      " [  73 1221]]\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.51      0.25      0.94      0.34      0.49      0.22       302\n",
      "          1       0.84      0.94      0.25      0.89      0.49      0.25      1294\n",
      "\n",
      "avg / total       0.78      0.81      0.38      0.79      0.49      0.25      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "X_resampled, y_resampled = SMOTE(random_state=1,\n",
    "sampling_strategy='auto').fit_resample(\n",
    "   X_train_scaled, y_train)\n",
    "\n",
    "model =  RandomForestClassifier(n_estimators=128, random_state=0).fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db93a820",
   "metadata": {},
   "source": [
    "## UnderSampling (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "021172b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[194 108]\n",
      " [436 858]]\n",
      "0.6527221920836873\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.31      0.64      0.66      0.42      0.65      0.43       302\n",
      "          1       0.89      0.66      0.64      0.76      0.65      0.43      1294\n",
      "\n",
      "avg / total       0.78      0.66      0.65      0.69      0.65      0.43      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "ros = RandomUnderSampler(random_state=1)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=128, random_state=0).fit(X_resampled, y_resampled)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d97ac77",
   "metadata": {},
   "source": [
    "## Clusters (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "e8c5b101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 260   42]\n",
      " [1060  234]]\n",
      "0.5208808868235463\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.20      0.86      0.18      0.32      0.39      0.17       302\n",
      "          1       0.85      0.18      0.86      0.30      0.39      0.15      1294\n",
      "\n",
      "avg / total       0.72      0.31      0.73      0.30      0.39      0.15      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=1)\n",
    "X_resampled, y_resampled = cc.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=128, random_state=0)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97b52ed",
   "metadata": {},
   "source": [
    "## SMOTEEN (RFC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "21a6175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[154 148]\n",
      " [304 990]]\n",
      "0.6375016633059357\n",
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "          0       0.34      0.51      0.77      0.41      0.62      0.38       302\n",
      "          1       0.87      0.77      0.51      0.81      0.62      0.40      1294\n",
      "\n",
      "avg / total       0.77      0.72      0.56      0.74      0.62      0.40      1596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.combine import SMOTEENN\n",
    "smote_enn = SMOTEENN(random_state=0)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=128, random_state=0)\n",
    "model.fit(X_resampled, y_resampled)\n",
    "\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(balanced_accuracy_score(y_test, y_pred))\n",
    "print(classification_report_imbalanced(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3dadd",
   "metadata": {},
   "source": [
    "## scikit-learn Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b220d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/\n",
    "\n",
    "# the gradient of the loss is estimated each sample at a time and the model is updated along the way with \n",
    "# a decreasing strength schedule (aka learning rate).\n",
    "\n",
    "# Linear classifiers (SVM, logistic regression, etc.) with SGD training.\n",
    "\n",
    "# The log loss gives logistic regression, a probabilistic classifier. \n",
    "\n",
    "# For best results using the default learning rate schedule, the data should have zero mean and unit variance.\n",
    "\n",
    "# The sag solver uses Stochastic Average Gradient descent 6. It is faster than other solvers for large datasets, \n",
    "# when both the number of samples and the number of features are large.\n",
    "# sag and saga fast convergence is only guaranteed on features with approximately the same scale.\n",
    "\n",
    "# The regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector.\n",
    "\n",
    "# The classes SGDClassifier and SGDRegressor provide functionality to fit linear models for classification and \n",
    "# regression using different (convex) loss functions and different penalties. E.g., with loss=\"log\", SGDClassifier \n",
    "# fits a logistic regression model, while with loss=\"hinge\" it fits a linear support vector machine (SVM).\n",
    "\n",
    "\n",
    "# The class SGDRegressor implements a plain stochastic gradient descent learning routine which supports different \n",
    "# loss functions and penalties to fit linear regression models. SGDRegressor is well suited for regression problems \n",
    "# with a large number of training samples (> 10.000), for other problems we recommend Ridge, Lasso, or ElasticNet.\n",
    "\n",
    "\n",
    "# Given a set of training examples where and (for classification), \n",
    "# our goal is to learn a linear scoring function with model parameters \n",
    "# and intercept . In order to make predictions for binary classification, \n",
    "# we simply look at the sign of . To find the model parameters, we minimize \n",
    "# the regularized training error\n",
    "\n",
    "# where L is a loss function that measures model (mis)fit and  is a regularization term (aka penalty) that\n",
    "# penalizes model complexity; alpha > 0 is a non-negative hyperparameter that controls the regularization strength.\n",
    "# Log: equivalent to Logistic Regression.\n",
    "\n",
    "\n",
    "\n",
    "# SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, \n",
    "#               tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, \n",
    "#               learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, \n",
    "#               n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
    "\n",
    "# SGDRegressor(loss='squared_error', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, \n",
    "#              max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, random_state=None, \n",
    "#              learning_rate='invscaling', eta0=0.01, power_t=0.25, early_stopping=False, validation_fraction=0.1, \n",
    "#              n_iter_no_change=5, warm_start=False, average=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
